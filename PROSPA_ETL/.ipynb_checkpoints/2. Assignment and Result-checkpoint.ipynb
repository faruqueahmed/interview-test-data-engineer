{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Prospa]\n",
    "-------------\n",
    "\n",
    "How to do it?\n",
    "-----------------------\n",
    "Fork this repo, build your ETL process and commit the code with your answers. Open a Pull Request and send us a message highlighting the test is completed.\n",
    "\n",
    "#### Rules\n",
    "* it must come with step by step instructions to run the code.\n",
    "* please, be mindful that your code might be moved or deleted after we analyse the PR. \n",
    "* use the best practices\n",
    "* be able to explain from the ground up the whole process on face to face interview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Faruque]\n",
    "-------------\n",
    "\n",
    "* Please run the Jupyter Notebook (1 The small ETL project.ipynb) . The Notebook contains steps to run the code. \n",
    "* Solution contains suitable best practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Faruque]\n",
    "-------------\n",
    "\n",
    "### Tools and Technologies\n",
    "Due to PROSPA recommendations following tools and technologies are used\n",
    "\n",
    "Programming Language : Python 3\n",
    "Database: Mysql 5.X\n",
    "\n",
    "For Bonus point SQLITE database is used\n",
    "\n",
    "Reporting and Visualization: Jupyter Notebook \n",
    "\n",
    "Development and tested OS- Ubuntu 18.04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Prospa]\n",
    "-------------\n",
    "\n",
    "The small ETL project\n",
    "--------- \n",
    "\n",
    "1. The data for this exercise can be found on the `data.zip` file. Can you describe the file format?\n",
    "\n",
    "[Faruque]\n",
    "-------------\n",
    "\n",
    "\n",
    "1. `*.tlb` files contains Pipe(|) Delimeted structured Text file.  One extra pipe was found at the end of each line. Therefore, during data processing that pipe was dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Prospa]\n",
    "-------------\n",
    "\n",
    "**Super Bonus**: generate your own data through the instructions on the encoded file `bonus_etl_data_gen.txt`.\n",
    "To get the bonus points, please encoded the file with the instructions were used to generate the files.\n",
    "\n",
    "[Faruque]\n",
    "-------------\n",
    "\n",
    "\n",
    "`bonus_etl_data_gen.txt` is an encoded file. `cat  ../bonus_etl_data_gen.txt | base64 --decode` command is used to decrypt. I started to develop the solution on MySQL , to get this extra **Super Bonus** i installed SQLITE as per the istructions and a new dataset was prepared with the scale factor of 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the instruction on https://github.com/lovasoa/TPCH-sqlite to generate your data files.\r\n",
      "The data.zip file were generated with scale factor of 0.01\r\n",
      "Please, encode your file with the instruction you used to generate the data files.\r\n"
     ]
    }
   ],
   "source": [
    "!cat  ../bonus_etl_data_gen.txt | base64 --decode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Prospa]\n",
    "-------------\n",
    "\n",
    "2. Code you scripts to load the data into a database.\n",
    "\n",
    "\n",
    "[Faruque]\n",
    "-------------\n",
    "\n",
    "  `./python/start_etl.py` scripts perform the relevent ETL job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Prospa]\n",
    "-------------\n",
    "3. Design a star schema model which the data should flow.\n",
    "\n",
    "4. Build your process to load the data into the star schema \n",
    "\n",
    "**Bonus** point: \n",
    "- add a fields to classify the customer account balance in 3 groups \n",
    "- add revenue per line item \n",
    "- convert the dates to be distributed over the last 2 years\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[Faruque]\n",
    "-------------\n",
    "Star schema model is designed and releven Dimension and Fact tables were created. \n",
    "`./python/prepare_dim_fact_table.py\t` scripts is used to load data into star schema\n",
    "\n",
    "**Bonus** point: \n",
    "- a new table  called CUSTOMER_STATUS is created using following SQL where each customer is classified based on his account balence\n",
    "\n",
    "\n",
    "``` SELECT C1.C_CUSTKEY, C1.C_ACCTBAL  ,\n",
    "        CASE \n",
    "            WHEN C1.C_ACCTBAL < LOWERQUARTILE     THEN 'LOW'\n",
    "            WHEN C1.C_ACCTBAL < UPPERQUARTILE    THEN 'MID'\n",
    "            ELSE 'HIGH'\n",
    "        END AS C_STATUS\n",
    "    FROM CUSTOMER C1\n",
    "    CROSS JOIN (\n",
    "        SELECT \n",
    "        MIN(C_ACCTBAL) ,\n",
    "        MAX(C_ACCTBAL) ,\n",
    "        ((MIN(C_ACCTBAL) + AVG(C_ACCTBAL)) / 2) AS LOWERQUARTILE,\n",
    "        ((MAX(C_ACCTBAL) + AVG(C_ACCTBAL)) / 2) AS UPPERQUARTILE\n",
    "        FROM CUSTOMER\n",
    "        ) C2         ```\n",
    "        \n",
    "        \n",
    "-   ORDER_ITEM_FACT table contains revenue per line item, Following logic was used to calculate revenue per line item\n",
    "``` \n",
    " ( L_QUANTITY *L_EXTENDEDPRICE - L_DISCOUNT) LINEITEM_REVENUE  ,\n",
    "```\n",
    "- convert the dates to be distributed over the last 2 years\n",
    "\n",
    "If I understood the question properly, I need to convert all the date values. If the value is not within the last 2 years (20016 and 2017) then change the year value to 2016 or 2017. The logic I used to complete the task is  if the year value is even then replace the year(yyyy) with 2016 otherwise replace with 2017\n",
    "\n",
    "example -  1992-12-30  will be converted to 2016-12-30\n",
    "The logic used to have 2 years data  \n",
    "\n",
    "```   CASE\n",
    "        WHEN\n",
    "            MOD(YEAR(O_ORDERDATE), 2) = 0\n",
    "        THEN\n",
    "            STR_TO_DATE(CONCAT('2016', SUBSTRING(O_ORDERDATE, 5, 6)),\n",
    "                    '%Y-%m-%d')\n",
    "        ELSE STR_TO_DATE(CONCAT('2017', SUBSTRING(O_ORDERDATE, 5, 6)),\n",
    "                '%Y-%m-%d')\n",
    "    END AS ODATE2  \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Prospa]\n",
    "-------------\n",
    "\n",
    "\n",
    "5. How to schedule this process to run multiple times per day?\n",
    " \n",
    "**Bonus**: What to do if the data arrives in random order and times via streaming?\n",
    "\n",
    "\n",
    "[Faruque]\n",
    "-------------\n",
    "\n",
    "My development platform was Ubuntu 18.04. \n",
    "Using cron , the job can be scheduled. If we need to run the job every hour then the cron syntax would be\n",
    "\n",
    "`0 */1 * * * python /<scripts path>/python/start_etl.py` \n",
    "\n",
    "\n",
    "If the solution is deployed on DCOS then metronome can be used for scheduling the job. Apache Airflow can be another alternative.\n",
    "\n",
    "\n",
    "\n",
    "This ETL solution is developed for batch procssing and considered there will be no dealy in file landing.  To manages streaming solution current proposed tools are not sufficient. \n",
    "\n",
    "\n",
    "Design of the ETL depends on other lots of factors. If , for any reason, csv files arrive late, let say delay is 2 days. then Partition strategy , loading time and extra logic in the data procescing scripts are required. \n",
    "\n",
    "To handle Streaming data there are few solutions  available. We can use Flume , Kafka and Spark Streaming to solve streaming data loading.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Prospa]\n",
    "-------------\n",
    "\n",
    "\n",
    "6. How to deploy this code?\n",
    "\n",
    "**Bonus**: Can you make it to run on a container like process (Docker)? \n",
    "\n",
    "[Faruque]\n",
    "---------\n",
    "\n",
    "Yes, I can containarize the solution using Docker. I may have a relevent Docker file to share in the presentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Prospa]\n",
    "-------------\n",
    "\n",
    "Data Reporting\n",
    "-------\n",
    "One of the most important aspects to build a DWH is to deliver insights to end-users. Besides the question bellow, what extra insights you can think of can be generated from this dataset?\n",
    "\n",
    "Can you using the designed star schema (or if you prefer the raw data), generate SQL statements to answer the following questions:\n",
    "\n",
    "1. What are the top 5 nations in terms of revenue?\n",
    "\n",
    "2. From the top 5 nations, what is the most common shipping mode?\n",
    "\n",
    "3. What are the top selling months?\n",
    "\n",
    "4. Who are the top customer in terms of revenue and/or quantity?\n",
    "\n",
    "5. Compare the sales revenue of on current period against previous period?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. Code you scripts to load the data into a database.\n",
    "=====================================================\n",
    "As it is considered to prepare ETL process for Dataware house solution therefore database table partition selection is importat. Due to time constraint for Fact table partition is created. \n",
    "\n",
    "For VLDB like warehouse systems keeping the integrity constraint is expensive. It is expected On OLTP system the data is inserted having integrity constraint. Data which wiil be ingested is curated. Therefore, of large tables , intigrity constraints were dropped. Therefore original DDL scripts provided by PROSPA was updated. Two additional columns were introduced . One for dataingestion time and another for filename. Those 2 fields will help to debug any failure in future.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3. Design a star schema model which the data should flow.\n",
    "\n",
    "Star Schema is designd based on the project requirements. There is room of improvement for further reporting.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Bonus** point: \n",
    "- add a fields to classify the customer account balance in 3 groups \n",
    "\n",
    "Before creating the fact table it is important to load data as it is in the source image. \n",
    "\n",
    "for customer classification  a table is created. In production environment , this scripts can be executed monthly once to have the updated status of customer.\n",
    "\n",
    "\n",
    "- add revenue per line item \n",
    "\n",
    "Revenue is calculated based on the following formula\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- convert the dates to be distributed over the last 2 years\n",
    "if i understood the question properly, i need to convert all the date values, if  the value is not within the last 2 years (20018 and 2017) then change the year value to 2017 or 2018. To logic i used to complete the task is  if the year value is even then replace the year(yyyy) with 2018 otherwise replace with 2017\n",
    "\n",
    "example -  1992-12-30  will be converted to 2018-12-30\n",
    "The logic used to have 2 years data  - in the dataset minimum ye\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "5. How to schedule this process to run multiple times per day?\n",
    "Using cron , the job can be scheduled. If we need to run the job every hour then the cron syntax would be\n",
    "\n",
    "0 */1 * * * sh /<scripts path>/runETL.sh\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "**Bonus**: What to do if the data arrives in random order and times via streaming?\n",
    "\n",
    "The ETL solution is developed for batch procssing and considered there will be no dealy in file landing.  To manages streaming solution current proposed tools are the sufficient. \n",
    "\n",
    "\n",
    "Design of the ETL depends on other lots of factors. If , for any reason, csv files arrive late, let say delay is 2 days. the Partition \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
